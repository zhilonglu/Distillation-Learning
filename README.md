# Distillation Learning

All materials related to Distillation Learning

## Applications
### Recommendation System
* [2018-AAAI-Rocket launching: A universal and efficient framework for training well-performing light net]()
* [2018-KDD_Ranking distillation: Learning compact ranking models with high performance for recommender system]()
* [2020-KDD-Privileged Features Distillation at Taobao Recommendations]()
* 


## Codes
* [Alignahead: Online Cross-Layer Knowledge Extraction on Graph Neural Networks](https://github.com/GuoJY-eatsTG/Alignahead)
* [Knowledge Distillation Meets Self-Supervision](https://github.com/xuguodong03/SSKD)


## Open-source Implementations
* [Awesome Knowledge Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)
* [Awesome Knowledge-Distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)
* [Pytorch implementation of various Knowledge Distillation (KD) methods](https://github.com/AberHu/Knowledge-Distillation-Zoo)
* [A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility](https://github.com/peterliht/knowledge-distillation-pytorch)

