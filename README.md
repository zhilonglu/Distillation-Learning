# Distillation Learning

All materials related to Distillation Learning

## Applications
### Recommendation System
* [2018-AAAI-Rocket launching: A universal and efficient framework for training well-performing light net](https://github.com/zhougr1993/Rocket-Launching)
* [2018-KDD-Ranking distillation: Learning compact ranking models with high performance for recommender system](https://github.com/graytowne/rank_distill)
* 2020-KDD-Privileged Features Distillation at Taobao Recommendations
* 2020-CIKM-Ensembled CTR Prediction via Knowledge Distillation


## Codes
* [Alignahead: Online Cross-Layer Knowledge Extraction on Graph Neural Networks](https://github.com/GuoJY-eatsTG/Alignahead)
* [Knowledge Distillation Meets Self-Supervision](https://github.com/xuguodong03/SSKD)


## Open-source Implementations
* [Awesome Knowledge Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)
* [Awesome Knowledge-Distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)
* [Pytorch implementation of various Knowledge Distillation (KD) methods](https://github.com/AberHu/Knowledge-Distillation-Zoo)
* [A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility](https://github.com/peterliht/knowledge-distillation-pytorch)

